{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "import random\n",
    "import gc\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Configuring pandas display options\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Determining the default device based on availability\n",
    "def_device = (\n",
    "    'mps' if torch.backends.mps.is_available() \n",
    "    else 'cuda' if torch.cuda.is_available() \n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "def_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_filter_ids(file_path):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file, checks for NaN values in 'step' column grouped by 'series_id',\n",
    "    and returns a list of 'series_id' values that do not contain NaNs.\n",
    "    \n",
    "    :param file_path: Path to the CSV file.\n",
    "    :return: List of series IDs without NaN values in the 'step' column.\n",
    "    \"\"\"\n",
    "    # Load data from CSV\n",
    "    train_events = pd.read_csv(file_path)\n",
    "\n",
    "    # Group by 'series_id' and check for NaN values in 'step' column\n",
    "    series_has_nan = train_events.groupby('series_id')['step'].apply(lambda x: x.isnull().any())\n",
    "\n",
    "    # Get list of series IDs that do not contain NaN values\n",
    "    train_ids = series_has_nan[~series_has_nan].index.tolist()\n",
    "\n",
    "    return train_ids\n",
    "\n",
    "# Usage example:\n",
    "file_path = \"../data/train_events.csv\"\n",
    "train_ids = load_data_and_filter_ids(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_light_series(series_ids):\n",
    "    \"\"\"\n",
    "    Fetches and processes a dataset for the given series IDs.\n",
    "\n",
    "    :param series_ids: List of series IDs to fetch.\n",
    "    :return: Processed DataFrame with added features.\n",
    "    \"\"\"\n",
    "    print(f'Fetching series IDs: {series_ids} \\n')\n",
    "    file_path = \"../data/zzzs-lightweight-training-dataset-target/Zzzs_train.parquet\"\n",
    "    multi_series = pd.read_parquet(file_path, filters=[('series_id', 'in', series_ids)])\n",
    "    multi_series = multi_series.astype({'series_id': 'category', 'step': 'int16', 'awake': 'int16'})\n",
    "    multi_series = add_features(multi_series)\n",
    "\n",
    "    return multi_series\n",
    "\n",
    "def add_features(df):\n",
    "    \"\"\"\n",
    "    Adds various features to the DataFrame.\n",
    "\n",
    "    :param df: DataFrame to which features are added.\n",
    "    :return: DataFrame with added features.\n",
    "    \"\"\"\n",
    "    df = add_time_features(df)\n",
    "    df = add_interaction_features(df)\n",
    "    df = add_rolling_features(df, periods=6)  # 1/2 minute\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_time_features(df):\n",
    "    \"\"\" Adds time-related features to the DataFrame. \"\"\"\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n",
    "    return df\n",
    "\n",
    "def add_interaction_features(df):\n",
    "    \"\"\" Adds interaction features to the DataFrame. \"\"\"\n",
    "    df[\"anglez_times_enmo\"] = abs(df[\"anglez\"]) * df[\"enmo\"]\n",
    "    return df\n",
    "\n",
    "def add_rolling_features(df, periods):\n",
    "    \"\"\" Adds rolling features to the DataFrame. \"\"\"\n",
    "    # Define operations to be applied\n",
    "    operations = [\"mean\", \"min\", \"max\", \"std\"]\n",
    "    columns = [\"anglez\", \"enmo\"]\n",
    "\n",
    "    for column in columns:\n",
    "        for operation in operations:\n",
    "            df[f\"{column}_{operation}\"] = (\n",
    "                df[column].rolling(periods, center=True).agg(operation).bfill().ffill().astype('float32')\n",
    "            )\n",
    "\n",
    "        # Differential features\n",
    "        df[f\"{column}_diff\"] = (\n",
    "            df.groupby('series_id', observed=True)[column].diff(periods=periods).bfill()\n",
    "        )\n",
    "        df[f\"{column}_diff_rolling\"] = (\n",
    "            df[f\"{column}_diff\"].rolling(periods, center=True).mean().bfill().ffill().astype('float32')\n",
    "        )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching series IDs: ['08db4255286f', '0a96f4993bd7', '0cfc06c129cc', '1087d7b0ff2e', '10f8bc1f7b07', '18b61dd5aae8', '29c75c018220', '31011ade7c0a'] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11.5 s\n",
      "Wall time: 16.8 s\n",
      "memory usage:  219.76 MB\n"
     ]
    }
   ],
   "source": [
    "%time train_all = get_multi_light_series(train_ids[:8])\n",
    "print(f'memory usage: {train_all.memory_usage().sum() / 1024**2: .2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>anglez</th>\n",
       "      <th>enmo</th>\n",
       "      <th>awake</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>anglez_times_enmo</th>\n",
       "      <th>anglez_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>anglez_max</th>\n",
       "      <th>anglez_std</th>\n",
       "      <th>anglez_diff</th>\n",
       "      <th>anglez_diff_rolling</th>\n",
       "      <th>enmo_mean</th>\n",
       "      <th>enmo_min</th>\n",
       "      <th>enmo_max</th>\n",
       "      <th>enmo_std</th>\n",
       "      <th>enmo_diff</th>\n",
       "      <th>enmo_diff_rolling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08db4255286f</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-05 14:00:00+00:00</td>\n",
       "      <td>-30.85</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.38</td>\n",
       "      <td>-33.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.85</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08db4255286f</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-05 14:00:05+00:00</td>\n",
       "      <td>-34.18</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.51</td>\n",
       "      <td>-33.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.85</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08db4255286f</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-11-05 14:00:10+00:00</td>\n",
       "      <td>-33.88</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.64</td>\n",
       "      <td>-33.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.85</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08db4255286f</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-11-05 14:00:15+00:00</td>\n",
       "      <td>-34.28</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-33.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.85</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08db4255286f</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-11-05 14:00:20+00:00</td>\n",
       "      <td>-34.39</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>-33.69</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.51</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      series_id  step                 timestamp  anglez  enmo  awake  hour  \\\n",
       "0  08db4255286f     0 2018-11-05 14:00:00+00:00  -30.85  0.04      1    14   \n",
       "1  08db4255286f     1 2018-11-05 14:00:05+00:00  -34.18  0.04      1    14   \n",
       "2  08db4255286f     2 2018-11-05 14:00:10+00:00  -33.88  0.05      1    14   \n",
       "3  08db4255286f     3 2018-11-05 14:00:15+00:00  -34.28  0.07      1    14   \n",
       "4  08db4255286f     4 2018-11-05 14:00:20+00:00  -34.39  0.08      1    14   \n",
       "\n",
       "   dayofweek  anglez_times_enmo  anglez_mean  ...  anglez_max  anglez_std  \\\n",
       "0          0               1.38       -33.75  ...      -30.85        1.46   \n",
       "1          0               1.51       -33.75  ...      -30.85        1.46   \n",
       "2          0               1.64       -33.75  ...      -30.85        1.46   \n",
       "3          0               2.33       -33.75  ...      -30.85        1.46   \n",
       "4          0               2.64       -33.69  ...      -30.51        1.60   \n",
       "\n",
       "   anglez_diff  anglez_diff_rolling  enmo_mean  enmo_min  enmo_max  enmo_std  \\\n",
       "0         0.33                 0.33       0.06      0.04      0.08      0.01   \n",
       "1         0.33                 0.33       0.06      0.04      0.08      0.01   \n",
       "2         0.33                 0.33       0.06      0.04      0.08      0.01   \n",
       "3         0.33                 0.33       0.06      0.04      0.08      0.01   \n",
       "4         0.33                 0.33       0.07      0.04      0.11      0.02   \n",
       "\n",
       "   enmo_diff  enmo_diff_rolling  \n",
       "0       0.06               0.06  \n",
       "1       0.06               0.06  \n",
       "2       0.06               0.06  \n",
       "3       0.06               0.06  \n",
       "4       0.06               0.06  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features_and_extract_target(df, feature_names, target_name):\n",
    "    \"\"\"\n",
    "    Scales the features of the dataset and extracts the target variable.\n",
    "\n",
    "    :param df: DataFrame containing the dataset.\n",
    "    :param feature_names: List of feature names to be scaled.\n",
    "    :param target_name: Name of the target variable.\n",
    "    :return: Tuple of scaled features array and target variable array.\n",
    "    \"\"\"\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Scale the features\n",
    "    df_features_scaled = scaler.fit_transform(df[feature_names])\n",
    "\n",
    "    # Extract the target variable\n",
    "    df_target = df[target_name].values\n",
    "\n",
    "    return df_features_scaled, df_target\n",
    "\n",
    "# Example usage:\n",
    "features = [\"step\", \"hour\", \"dayofweek\", \"anglez_times_enmo\",\n",
    "            \"anglez\", \"anglez_diff\", \"anglez_mean\", \"anglez_min\", \"anglez_max\", \"anglez_std\", \"anglez_diff_rolling\",\n",
    "            \"enmo\", \"enmo_diff\", \"enmo_mean\", \"enmo_min\", \"enmo_max\", \"enmo_std\", \"enmo_diff_rolling\"]\n",
    "target = 'awake'\n",
    "\n",
    "# Assuming 'train_all' is your DataFrame\n",
    "df_train_X_scaled, df_train_y = scale_features_and_extract_target(train_all, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_and_split(df_features, df_target, split_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Converts feature and target dataframes into PyTorch tensors and splits them into training and validation sets.\n",
    "\n",
    "    :param df_features: DataFrame or array containing the feature data.\n",
    "    :param df_target: DataFrame or array containing the target data.\n",
    "    :param split_ratio: Float representing the proportion of the dataset to include in the train split.\n",
    "    :return: Tuples of tensors (X_train, y_train), (X_val, y_val).\n",
    "    \"\"\"\n",
    "    # Convert to PyTorch tensors\n",
    "    X = tensor(df_features, dtype=torch.float32)\n",
    "    y = tensor(df_target, dtype=torch.long)\n",
    "\n",
    "    # Split the data\n",
    "    split_index = int(len(X) * split_ratio)\n",
    "    X_train, X_val = X[:split_index], X[split_index:]\n",
    "    y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes (X, y): torch.Size([2275920, 18]) torch.Size([2275920])\n",
      "Validation shapes (X, y): torch.Size([568980, 18]) torch.Size([568980])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(X_train, y_train), (X_val, y_val) = prepare_data_and_split(df_train_X_scaled, df_train_y)\n",
    "\n",
    "# Checking the shapes\n",
    "print(\"Train shapes (X, y):\", X_train.shape, y_train.shape)\n",
    "print(\"Validation shapes (X, y):\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\" Custom Dataset for handling time series data. \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, batch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates DataLoader objects for training and validation datasets.\n",
    "\n",
    "    :param train_dataset: Training dataset of type TimeSeriesDataset.\n",
    "    :param val_dataset: Validation dataset of type TimeSeriesDataset.\n",
    "    :param batch_size: Batch size for the DataLoader.\n",
    "    :param shuffle: Boolean indicating whether to shuffle the dataset.\n",
    "    :return: Tuple of DataLoader objects for training and validation datasets.\n",
    "    \"\"\"\n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_dl, val_dl\n",
    "\n",
    "# Example usage:\n",
    "batch_size = 12*60  # 1 hour\n",
    "train_ds = TimeSeriesDataset(X_train, y_train)\n",
    "val_ds = TimeSeriesDataset(X_val, y_val)\n",
    "\n",
    "train_dl, val_dl = create_dataloaders(train_ds, val_ds, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "\n",
    "        :param input_size: Number of input features.\n",
    "        :param hidden_size: Number of features in the hidden state of the LSTM.\n",
    "        :param num_layers: Number of recurrent layers.\n",
    "        :param output_size: Number of output features (size of output tensor).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Output size is doubled for bidirectional LSTM\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM.\n",
    "\n",
    "        :param x: Input tensor.\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        # Reshape input to 3D tensor for LSTM\n",
    "        x = x[:, None, :]  \n",
    "\n",
    "        # LSTM output\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Passing the output through the fully connected layer\n",
    "        out = self.fc(self.relu(out[:, -1, :])) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the model.\n",
    "\n",
    "    :param outputs: Model predictions.\n",
    "    :param labels: Ground truth labels.\n",
    "    :return: Accuracy as a float.\n",
    "    \"\"\"\n",
    "    predictions = outputs.argmax(dim=1)\n",
    "    correct = (predictions == labels)\n",
    "    return correct.float().mean()\n",
    "\n",
    "def train(epochs, model, loss_func, optimizer, train_loader, valid_loader, device):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the model.\n",
    "\n",
    "    :param epochs: Number of epochs to train.\n",
    "    :param model: The neural network model.\n",
    "    :param loss_func: Loss function.\n",
    "    :param optimizer: Optimizer.\n",
    "    :param train_loader: DataLoader for training data.\n",
    "    :param valid_loader: DataLoader for validation data.\n",
    "    :param device: Device to run the model on.\n",
    "    :return: Tuple of final loss and accuracy.\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/ Training'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_loss, total_acc, count = 0., 0., 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(valid_loader, desc=f'Epoch {epoch+1}/ Evaluation'):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                count += len(inputs)\n",
    "                total_loss += loss_func(outputs, labels).item() * len(inputs)\n",
    "                total_acc += accuracy(outputs, labels).item() * len(inputs)\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Loss: {total_loss/count:.2f}, Accuracy: {total_acc/count:.2f}')\n",
    "\n",
    "    return total_loss / count, total_acc / count\n",
    "\n",
    "def predict(model, x, device):\n",
    "    \"\"\"\n",
    "    Makes predictions with the model.\n",
    "\n",
    "    :param model: The neural network model.\n",
    "    :param x: Input tensor.\n",
    "    :param device: Device to run the model on.\n",
    "    :return: Tuple of softmax scores and predicted values.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        outputs = model(x)\n",
    "        scores = F.log_softmax(outputs, -1).exp()\n",
    "        predictions = scores.argmax(dim=1)\n",
    "\n",
    "    return scores.cpu(), predictions.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(18, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Architecture\n",
    "input_size = len(features)\n",
    "hidden_size = 32 # like 1 2 4 32 64\n",
    "num_layers = 2\n",
    "output_size = 2\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "model.to(def_device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/ Training: 100%|██████████| 3161/3161 [00:29<00:00, 106.85it/s]\n",
      "Epoch 1/ Evaluation: 100%|██████████| 791/791 [00:07<00:00, 102.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.18, Accuracy: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/ Training: 100%|██████████| 3161/3161 [00:29<00:00, 106.40it/s]\n",
      "Epoch 2/ Evaluation: 100%|██████████| 791/791 [00:07<00:00, 102.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.15, Accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/ Training: 100%|██████████| 3161/3161 [00:29<00:00, 106.23it/s]\n",
      "Epoch 3/ Evaluation: 100%|██████████| 791/791 [00:07<00:00, 105.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.14, Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/ Training: 100%|██████████| 3161/3161 [00:29<00:00, 106.29it/s]\n",
      "Epoch 4/ Evaluation: 100%|██████████| 791/791 [00:07<00:00, 104.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.13, Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/ Training: 100%|██████████| 3161/3161 [00:29<00:00, 106.64it/s]\n",
      "Epoch 5/ Evaluation: 100%|██████████| 791/791 [00:07<00:00, 102.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.13, Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/ Training: 100%|██████████| 3161/3161 [00:29<00:00, 106.77it/s]\n",
      "Epoch 6/ Evaluation: 100%|██████████| 791/791 [00:07<00:00, 103.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.13, Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/ Training: 100%|██████████| 3161/3161 [00:29<00:00, 106.85it/s]\n",
      "Epoch 7/ Evaluation: 100%|██████████| 791/791 [00:07<00:00, 99.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.12, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/ Training: 100%|██████████| 3161/3161 [00:29<00:00, 107.42it/s]\n",
      "Epoch 8/ Evaluation: 100%|██████████| 791/791 [00:08<00:00, 92.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.12, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/ Training: 100%|██████████| 3161/3161 [00:29<00:00, 108.22it/s]\n",
      "Epoch 9/ Evaluation: 100%|██████████| 791/791 [00:08<00:00, 95.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.12, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/ Training: 100%|██████████| 3161/3161 [00:29<00:00, 108.51it/s]\n",
      "Epoch 10/ Evaluation: 100%|██████████| 791/791 [00:08<00:00, 95.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.12, Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss,acc = train(10, model, loss_func, opt, train_dl, val_dl, def_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleep_states",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
