{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Child Mind Sleep States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is adapted from/based on a script by Daniel Phalen available at: \n",
    "https://www.kaggle.com/code/danielphalen/cmss-grunet-train \n",
    "\n",
    "For the architecture a mix of GRU cells and UNET architecture. We are going to predict the the critical points, i.e. where the onset and wakeup events happen.  We will take as the target a gaussian of a tunable width centered around the actual point.\n",
    "\n",
    "Modifications were made to fit the specific requirements of this project.\n",
    "\n",
    "Cross Entropy loss from torch.  \n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "https://www.kaggle.com/code/werus23/sleep-critical-point-train/notebook\n",
    "\n",
    "https://www.kaggle.com/code/werus23/sleep-critical-point-infer?scriptVersionId=147143158\n",
    "\n",
    "https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states/discussion/441470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import ctypes\n",
    "import copy\n",
    "import gc\n",
    "import math\n",
    "import platform\n",
    "import random\n",
    "from math import exp, pi, sqrt\n",
    "\n",
    "# Third-party imports\n",
    "import dateutil.relativedelta as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import Markdown\n",
    "from plotly import __version__\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.tools import FigureFactory as FF\n",
    "from pyarrow.parquet import ParquetFile\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import average_precision_score\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Local module imports\n",
    "import event_detection_ap as mapmetric\n",
    "\n",
    "# Setup and configurations\n",
    "plt.style.use(\"ggplot\")\n",
    "init_notebook_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapmetric.series_id_column_name = 'series_id'\n",
    "mapmetric.time_column_name = 'step'\n",
    "mapmetric.event_column_name = 'event'\n",
    "mapmetric.score_column_name = 'score'\n",
    "mapmetric.use_scoring_intervals = False\n",
    "tolerance_intervals = [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]\n",
    "tolerances = {'wakeup': tolerance_intervals, 'onset': tolerance_intervals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PATHS:\n",
    "    # Set this variable to True to use Kaggle paths, False for local paths\n",
    "    USE_KAGGLE_PATHS = False\n",
    "\n",
    "    if USE_KAGGLE_PATHS:\n",
    "        # Paths for Kaggle\n",
    "        MAIN_DIR = \"/kaggle/input/child-mind-institute-detect-sleep-states/\"\n",
    "        SPLIT_DIR = \"/kaggle/input/child-sleep-mind-split-train/\"\n",
    "    else:\n",
    "        # Paths for local environment\n",
    "        MAIN_DIR = \"../data/\"\n",
    "        SPLIT_DIR = f\"{MAIN_DIR}split-train/\"\n",
    "\n",
    "    # Common paths\n",
    "    SUBMISSION = f\"{MAIN_DIR}sample_submission.csv\"\n",
    "    TRAIN_EVENTS = f\"{MAIN_DIR}train_events.csv\"\n",
    "    TRAIN_SERIES = f\"{MAIN_DIR}train_series.parquet\"\n",
    "    TEST_SERIES = f\"{MAIN_DIR}test_series.parquet\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_series_filename(series_id):\n",
    "        return f\"{PATHS.SPLIT_DIR}{series_id}_test_series.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEMO_MODE = False\n",
    "    VERBOSE = True\n",
    "    SEED = 42\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_fix_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for generating random numbers to ensure reproducibility.\n",
    "    It affects Python's `random`, NumPy's random number generator, and PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "        seed (int): The seed value for random number generators. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Seed Python's built-in random number generator\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Seed NumPy's random number generator\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Seed PyTorch's random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        # If CUDA is available, also seed the CUDA random number generator\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        # Uncomment the following lines if you need deterministic behavior in PyTorch,\n",
    "        # but be aware this can impact performance.\n",
    "        # torch.backends.cudnn.deterministic = True\n",
    "        # torch.use_deterministic_algorithms(True)\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch_fix_seed(CFG.SEED)  # Replace 'CFG.SEED' with the actual seed value you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    \"\"\"\n",
    "    Frees up memory in the environment. \n",
    "    Performs garbage collection for Python and clears PyTorch's CUDA cache.\n",
    "    On Linux systems, it also attempts to trim the malloc heap.\n",
    "    \"\"\"\n",
    "    # Garbage collection for Python\n",
    "    gc.collect()\n",
    "\n",
    "    # Clear PyTorch's CUDA cache if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Memory trimming specific to Linux systems\n",
    "    if platform.system() == \"Linux\":\n",
    "        try:\n",
    "            # Load the C standard library\n",
    "            libc = ctypes.CDLL(\"libc.so.6\")\n",
    "            # Call malloc_trim from the C library\n",
    "            libc.malloc_trim(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during malloc_trim: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Train Events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events = pd.read_csv(PATHS.TRAIN_EVENTS)\n",
    "print(len(train_events))\n",
    "print(train_events.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_continuous_event_sequence(gdf):\n",
    "    \"\"\"\n",
    "    Finds the longest continuous subsequence in a dataset where an event occurs each night.\n",
    "\n",
    "    Args:\n",
    "    gdf (DataFrame): A pandas DataFrame containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple (start_night, end_night) indicating the start and end of the longest subsequence.\n",
    "    \"\"\"\n",
    "    # Extract nights where both 'onset' and 'wakeup' events occurred\n",
    "    continuous_nights = sorted(\n",
    "        list(\n",
    "            set(gdf[gdf['event'] == 'onset'].dropna().night.unique()) &\n",
    "            set(gdf[gdf['event'] == 'wakeup'].dropna().night.unique())\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Initialize variables for tracking the longest sequence\n",
    "    longest_sequence_start = -1\n",
    "    longest_sequence_end = -1\n",
    "    current_sequence_start = -1\n",
    "    current_sequence_end = -1\n",
    "    max_sequence_length = 0\n",
    "    last_night = current_sequence_start\n",
    "\n",
    "    # Iterate through each night in the continuous nights\n",
    "    for night in continuous_nights:\n",
    "        if night == last_night + 1:\n",
    "            # Continue the current sequence\n",
    "            current_sequence_end = night\n",
    "            last_night = night\n",
    "        else:\n",
    "            # Calculate the length of the completed sequence\n",
    "            sequence_length = current_sequence_end - current_sequence_start\n",
    "            if sequence_length > max_sequence_length:\n",
    "                # Update the longest sequence\n",
    "                longest_sequence_start, longest_sequence_end = current_sequence_start, current_sequence_end\n",
    "                max_sequence_length = sequence_length\n",
    "\n",
    "            # Start a new sequence\n",
    "            current_sequence_start = night\n",
    "            current_sequence_end = night\n",
    "            last_night = night\n",
    "\n",
    "    # Check and update for the last sequence\n",
    "    sequence_length = current_sequence_end - current_sequence_start\n",
    "    if sequence_length > max_sequence_length:\n",
    "        longest_sequence_start, longest_sequence_end = current_sequence_start, current_sequence_end\n",
    "\n",
    "    return longest_sequence_start, longest_sequence_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_continuous_series(train_events):\n",
    "    \"\"\"\n",
    "    Identifies series with continuous data in the 'train_events' DataFrame.\n",
    "    A series is dropped if it has no non-null data or if it doesn't have a continuous range.\n",
    "\n",
    "    Parameters:\n",
    "        train_events (DataFrame): The DataFrame containing the event data.\n",
    "\n",
    "    Returns:\n",
    "        drop_series (list): List of series_ids to be dropped.\n",
    "        continuous (dict): Dictionary of series_ids with their continuous start and end.\n",
    "    \"\"\"\n",
    "    drop_series = []\n",
    "    continuous = {}\n",
    "\n",
    "    for series_id, gdf in train_events.groupby('series_id'):\n",
    "        # Drop series if all values are NaN\n",
    "        if gdf.dropna().empty:\n",
    "            drop_series.append(series_id)\n",
    "            continue\n",
    "\n",
    "        # Check for the longest continuous range\n",
    "        start, end = find_longest_continuous_event_sequence(gdf)\n",
    "        \n",
    "        # Drop series if there's no continuous range\n",
    "        if end - start == 0:\n",
    "            drop_series.append(series_id)\n",
    "        else:\n",
    "            continuous[series_id] = (start, end)\n",
    "\n",
    "    print(f'Drop {len(drop_series)} series.')\n",
    "    return drop_series, continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_series, continuous = filter_continuous_series(train_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_ids = train_events.series_id.unique()\n",
    "len(series_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(test_ds, index, model, max_chunk_size=1024):\n",
    "    \"\"\"\n",
    "    Generates predictions for a specific index in the test dataset using a neural network model.\n",
    "\n",
    "    Parameters:\n",
    "        test_ds (Dataset): The test dataset.\n",
    "        index (int): The index of the data to be predicted.\n",
    "        model (torch.nn.Module): The neural network model for generating predictions.\n",
    "        max_chunk_size (int, optional): The maximum size of data chunks to process at a time. Default is 1024.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the predictions.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # Load the data from the test dataset\n",
    "        X = test_ds[index]\n",
    "\n",
    "        # Initialize a tensor to store predictions\n",
    "        pred = torch.zeros(X.shape).to(CFG.DEVICE, non_blocking=True)\n",
    "\n",
    "        # Initialize the hidden state (if applicable)\n",
    "        hidden_state = None\n",
    "\n",
    "        # Get the sequence length of the data\n",
    "        seq_len = X.shape[0]\n",
    "\n",
    "        # Process the data in chunks\n",
    "        for j in range(0, seq_len, max_chunk_size):\n",
    "            # Define the end index of the chunk\n",
    "            end_idx = min(j + max_chunk_size, seq_len)\n",
    "\n",
    "            # Extract the chunk of data and move it to the specified device\n",
    "            X_chunk = X[j:end_idx].float().to(CFG.DEVICE, non_blocking=True)\n",
    "\n",
    "            # Get predictions and the new hidden state from the model\n",
    "            y_pred, hidden_state = model(X_chunk, hidden_state)\n",
    "\n",
    "            # Detach the hidden state to avoid backpropagation through it\n",
    "            hidden_state = [h.detach() for h in hidden_state]\n",
    "\n",
    "            # Store the predictions\n",
    "            pred[j:end_idx, :] = y_pred\n",
    "\n",
    "            # Clean up to save memory\n",
    "            del X_chunk, y_pred\n",
    "\n",
    "        # Free up memory\n",
    "        clean_memory()\n",
    "\n",
    "    # Convert the predictions to a DataFrame and apply softmax for probability distribution\n",
    "    predicted_df = pd.DataFrame(torch.softmax(pred.cpu(), dim=1).numpy(), columns=['wakeup_val', 'onset_val'])\n",
    "\n",
    "    # Return the DataFrame containing the predictions\n",
    "    return predicted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(res_df, target, SIGMA):\n",
    "    \"\"\"\n",
    "    Identifies events in the predicted data and calculates their scores based on the area under the curve.\n",
    "\n",
    "    Parameters:\n",
    "        res_df (pd.DataFrame): DataFrame containing the predictions.\n",
    "        target (str): The target column in the DataFrame.\n",
    "        SIGMA (float): A parameter used to define the span for calculating the score.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of events, each represented as [step, target, score].\n",
    "    \"\"\"\n",
    "    # Find a threshold value, set to 10% of the maximum value in the target column\n",
    "    q = res_df[target].max() * 0.1\n",
    "\n",
    "    # Filter rows where target column value is greater than the threshold\n",
    "    tmp = res_df.loc[res_df[target] > q].copy()\n",
    "\n",
    "    # Calculate the difference between consecutive steps\n",
    "    tmp['gap'] = tmp['step'].diff()\n",
    "\n",
    "    # Filter out rows where the gap is greater than a specified value (5*5 in this case)\n",
    "    tmp = tmp[tmp['gap'] > 5*5]\n",
    "\n",
    "    # Initialize an empty list to store results\n",
    "    res = []\n",
    "\n",
    "    # Iterate through the filtered DataFrame to find local maxima\n",
    "    for i in range(len(tmp) + 1):\n",
    "        # Determine the start and end indices for each potential event\n",
    "        start_i = 0 if i == 0 else tmp['step'].iloc[i-1]\n",
    "        end_i = tmp['step'].iloc[i] if i < len(tmp) else res_df['step'].max()\n",
    "\n",
    "        # Select the data between the start and end indices\n",
    "        v = res_df.loc[(res_df['step'] > start_i) & (res_df['step'] < end_i)]\n",
    "\n",
    "        # Check if the maximum value in the selected range is above the threshold\n",
    "        if v[target].max() > q:\n",
    "            # Find the index of the maximum value in the target column within the range\n",
    "            idx = v.idxmax()[target]\n",
    "\n",
    "            # Get the corresponding step value\n",
    "            step = v.loc[idx, 'step']\n",
    "\n",
    "            # Define the span for calculating the score\n",
    "            span = 3 * SIGMA\n",
    "\n",
    "            # Calculate the score as the sum of target values within the span\n",
    "            score = res_df.loc[(res_df['step'] > step - span) & (res_df['step'] < step + span), target].sum()\n",
    "\n",
    "            # Append the results to the list\n",
    "            res.append([step, target, score])\n",
    "\n",
    "    # Return the list of events with their scores\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(valid_ds, index, model, max_chunk_size=1024):\n",
    "    \"\"\"\n",
    "    Compares predicted and actual events for a given index in the validation dataset.\n",
    "\n",
    "    Parameters:\n",
    "        valid_ds (Dataset): The validation dataset.\n",
    "        index (int): The index of the data to be evaluated.\n",
    "        model (torch.nn.Module): The neural network model used for predictions.\n",
    "        max_chunk_size (int, optional): Maximum size of chunks to process at a time. Default is 1024.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing DataFrames for predicted and actual values.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations for performance\n",
    "    with torch.no_grad():\n",
    "        # Load data and target from the dataset\n",
    "        X, Y = valid_ds[index]\n",
    "\n",
    "        # Move target to the specified device\n",
    "        Y = Y.to(CFG.DEVICE, non_blocking=True)\n",
    "\n",
    "        # Initialize a tensor to store predictions\n",
    "        pred = torch.zeros(Y.shape).to(CFG.DEVICE, non_blocking=True)\n",
    "\n",
    "        # Initialize hidden state (if applicable)\n",
    "        hidden_state = None\n",
    "\n",
    "        # Get the sequence length\n",
    "        seq_len = X.shape[0]\n",
    "\n",
    "        # Process the data in chunks\n",
    "        for j in range(0, seq_len, max_chunk_size):\n",
    "            # Calculate the end index of the chunk\n",
    "            end_idx = min(j + max_chunk_size, seq_len)\n",
    "\n",
    "            # Extract the chunk and move it to the specified device\n",
    "            X_chunk = X[j:end_idx].float().to(CFG.DEVICE, non_blocking=True)\n",
    "\n",
    "            # Get predictions and the new hidden state from the model\n",
    "            y_pred, hidden_state = model(X_chunk, hidden_state)\n",
    "\n",
    "            # Detach the hidden state to avoid backpropagation through it\n",
    "            hidden_state = [h.detach() for h in hidden_state]\n",
    "\n",
    "            # Store the predictions\n",
    "            pred[j:end_idx, :] = y_pred\n",
    "\n",
    "            # Clean up to save memory\n",
    "            del X_chunk, y_pred\n",
    "\n",
    "        # Free up memory\n",
    "        clean_memory()\n",
    "\n",
    "    # Convert the predictions to a DataFrame, applying softmax for probability distribution\n",
    "    predicted_df = pd.DataFrame(torch.softmax(pred.cpu(), dim=1).numpy(), columns=['wakeup_val', 'onset_val'])\n",
    "\n",
    "    # Convert the actual values to a DataFrame\n",
    "    actual_df = pd.DataFrame(Y.cpu().numpy(), columns=['wakeup_val', 'onset_val'])\n",
    "\n",
    "    # Return the DataFrames for predicted and actual values\n",
    "    return predicted_df, actual_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepDatasetTrain(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for Child Mind Sleep States, focusing on raw anglez and enmo variables.\n",
    "\n",
    "    Attributes:\n",
    "        series_ids (list): List of series ids in the dataset.\n",
    "        events (pd.DataFrame): DataFrame containing event data.\n",
    "        len_mult (int): Length multiplier to ensure sequence length is a multiple of this value.\n",
    "        continuous (dict): Dictionary mapping series_id to start and end points for trimming.\n",
    "        sigma (float): Width of the distribution used for output calculations.\n",
    "    \"\"\"\n",
    "    def __init__(self, series_ids, events, len_mult, continuous=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Initializes the SleepDatasetTrain dataset.\n",
    "\n",
    "        Parameters:\n",
    "            series_ids (list): List of series ids in the dataset.\n",
    "            events (pd.DataFrame): DataFrame containing event data.\n",
    "            len_mult (int): Length multiplier to ensure sequence length is a multiple of this value.\n",
    "            continuous (dict, optional): Dictionary for trimming series. Default is None.\n",
    "            sigma (float, optional): Width of the distribution for output. Default is None.\n",
    "        \"\"\"\n",
    "        self.series_ids = series_ids\n",
    "        self.continuous = continuous\n",
    "        self.len_mult = len_mult\n",
    "        self.events = events\n",
    "        self.sigma = sigma if events is not None else None\n",
    "\n",
    "    def load_data(self, series_id):\n",
    "        \"\"\"\n",
    "        Loads and preprocesses data for a given series_id.\n",
    "\n",
    "        Parameters:\n",
    "            series_id (int): The series id for which to load the data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The preprocessed data for the specified series_id.\n",
    "        \"\"\"\n",
    "        # Load the data from a file\n",
    "        filename = PATHS.get_series_filename(series_id)\n",
    "        data = pd.read_parquet(filename)\n",
    "\n",
    "        # If events data is provided\n",
    "        if self.events is not None:\n",
    "            if self.continuous is not None:\n",
    "                start, end = self.continuous[series_id]\n",
    "            else:\n",
    "                start, end = 0, 1000000\n",
    "\n",
    "            gap = 6*60*12\n",
    "\n",
    "            # Filter events data\n",
    "            tmp = self.events[(self.events.series_id == series_id) & (self.events.night >= start) & (self.events.night <= end)]\n",
    "            data = data[(data.step > (tmp.step.min() - gap)) & (data.step < (tmp.step.max() + gap))]\n",
    "\n",
    "            # Join events data with the main data\n",
    "            data = data.set_index(['series_id', 'step']).join(tmp.set_index(['series_id', 'step'])[['event', 'night']]).reset_index()\n",
    "\n",
    "            # Calculate the distribution for each event type\n",
    "            if self.sigma == 0:\n",
    "                self.sigma = 1  # or some other small value\n",
    "            norm = 1 / np.sqrt(pi / self.sigma)\n",
    "\n",
    "            for evt in ['wakeup', 'onset']:\n",
    "                steps = data[data.event == evt]['step'].values\n",
    "                col = f'{evt}_val'\n",
    "                data[col] = 0.0\n",
    "                for i in steps:\n",
    "                    x = 0.5 * ((data.step.astype(np.int64) - i) / self.sigma) ** 2\n",
    "                    data[col] += np.exp(-x) * norm\n",
    "                data[col] /= data[col].sum()\n",
    "\n",
    "        # Trim the data to be a multiple of len_mult\n",
    "        n = int((len(data) // self.len_mult) * self.len_mult)\n",
    "        return data.iloc[:n]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of series ids in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.series_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Gets the item at the specified index.\n",
    "\n",
    "        Parameters:\n",
    "            index (int): The index of the item.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: A tuple containing feature tensor X and target tensor Y (if sigma is not None).\n",
    "        \"\"\"\n",
    "        # Load the data for the given series id\n",
    "        series_id = self.series_ids[index]\n",
    "        data = self.load_data(series_id)\n",
    "\n",
    "        # Extract features and convert them to a tensor\n",
    "        X = data[['anglez', 'enmo']].values.astype(np.float32)\n",
    "        X = torch.from_numpy(X)\n",
    "\n",
    "        # If sigma is provided, also prepare the target tensor\n",
    "        if self.sigma is not None:\n",
    "            Y = data[['wakeup_val', 'onset_val']].values.astype(np.float32)\n",
    "            Y = torch.from_numpy(Y)\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bi-directional LSTM with residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, n_layers=1, bidir=True):\n",
    "        super(ResidualBiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers,\n",
    "                            batch_first=True, bidirectional=bidir)\n",
    "        dir_factor = 2 if bidir else 1\n",
    "        self.fc1 = nn.Linear(hidden_size * dir_factor, hidden_size * dir_factor * 2)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size * dir_factor * 2)\n",
    "        self.fc2 = nn.Linear(hidden_size * dir_factor * 2, hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        # LSTM requires both hidden state (h) and cell state (c)\n",
    "        if h is None or c is None:\n",
    "            h, c = [None] * self.n_layers, [None] * self.n_layers\n",
    "        res, (new_h, new_c) = self.lstm(x, (h, c))\n",
    "        res = self.fc1(res)\n",
    "        res = self.ln1(res)\n",
    "        res = nn.functional.relu(res)\n",
    "        res = self.fc2(res)\n",
    "        res = self.ln2(res)\n",
    "        res = nn.functional.relu(res)\n",
    "        res = res + x  # skip connection\n",
    "        return res, new_h, new_c\n",
    "\n",
    "\n",
    "class MultiResidualBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, out_size, n_layers, bidir=True):\n",
    "        super(MultiResidualBiLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.fc_in = nn.Linear(input_size, hidden_size)\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "        self.res_bilstms = nn.ModuleList(\n",
    "            [\n",
    "                ResidualBiLSTM(hidden_size, n_layers=1, bidir=bidir)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        # Initialize hidden and cell states if not provided\n",
    "        if h is None or c is None:\n",
    "            h, c = [None] * self.n_layers, [None] * self.n_layers\n",
    "\n",
    "        x = self.fc_in(x)\n",
    "        x = self.ln(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        new_h, new_c = [], []\n",
    "        for i, res_bilstm in enumerate(self.res_bilstms):\n",
    "            x, new_hi, new_ci = res_bilstm(x, h[i], c[i])\n",
    "            new_h.append(new_hi)\n",
    "            new_c.append(new_ci)\n",
    "\n",
    "        x = self.fc_out(x)\n",
    "        return x, new_h, new_c  # Output + new hidden and cell states\n",
    "\n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size, kernel_size, stride, padding, dilation, use_layernorm, print_shape):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, hidden_size, kernel_size, stride, padding=padding, dilation=1)\n",
    "        self.ln = nn.LayerNorm(hidden_size) if use_layernorm else None\n",
    "        self.print_shape = print_shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x.transpose(-1,-2))\n",
    "        if self.print_shape:\n",
    "            print('After Conv', x.shape)\n",
    "        if self.ln is not None:\n",
    "            x = self.ln(x.transpose(-1, -2))\n",
    "        else:\n",
    "            x = x.transpose(-1,-2)\n",
    "        if self.print_shape:\n",
    "            print('After Layernorm', x.shape)\n",
    "        x = nn.functional.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class GRUNET(nn.Module):\n",
    "    def __init__(self, arch, in_channels, kernel_size, stride, dconv_padding, hidden_size, n_layers, bidir=True, print_shape=False):\n",
    "        \"\"\"\n",
    "        Initialize the GRUNET model.\n",
    "\n",
    "        Args:\n",
    "            arch (list of tuples): Architecture configuration for EncoderLayer.\n",
    "            in_channels (int): Number of input channels.\n",
    "            kernel_size (int): Size of the convolutional kernel.\n",
    "            stride (int): Stride value for convolution.\n",
    "            dconv_padding (int): Padding value for deconvolution.\n",
    "            hidden_size (int): Size of the hidden layer.\n",
    "            n_layers (int): Number of GRU layers.\n",
    "            bidir (bool): Whether to use bidirectional GRU.\n",
    "            print_shape (bool): Whether to print the shape of tensors during forward pass.\n",
    "        \"\"\"\n",
    "        super(GRUNET, self).__init__()\n",
    "        \n",
    "        self.input_size = in_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.out_size = out_size\n",
    "        self.n_layers = n_layers\n",
    "        self.padding = kernel_size//2\n",
    "        self.print_shape = print_shape\n",
    "        self.arch = arch\n",
    "        self.dilation = 1\n",
    "        assert arch[-1][1] == hidden_size\n",
    "\n",
    "        self.conv = nn.Sequential(*[EncoderLayer(in_chan, out_chan, ksize, stride=stride, padding=ksize//2, dilation=self.dilation, use_layernorm=True, print_shape=print_shape) for in_chan, out_chan, stride, ksize in self.arch])\n",
    "        self.res_bigrus = nn.ModuleList(\n",
    "            [\n",
    "                ResidualBiGRU(hidden_size, n_layers=1, bidir=bidir)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dconv = nn.Sequential(*sum([[nn.ConvTranspose1d(out_chan, in_chan, ksize, stride=stride, padding=ksize//2, dilation=self.dilation, output_padding=1), \n",
    "                                  nn.Conv1d(in_chan, in_chan, ksize, stride=1, padding=ksize//2, dilation=self.dilation), nn.ReLU(), \n",
    "                                  nn.Conv1d(in_chan, in_chan, ksize, stride=1, padding=ksize//2, dilation=self.dilation), nn.ReLU()] for in_chan, out_chan, stride, ksize in reversed(arch)], []))\n",
    "        self.output_layer = nn.Conv1d(2, 2, kernel_size=1, stride=1)\n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        # if we are at the beginning of a sequence (no hidden state)\n",
    "        init_shape = x.shape\n",
    "        if h is None:\n",
    "            # (re)initialize the hidden state\n",
    "            h = [None for _ in range(self.n_layers)]\n",
    "\n",
    "        if self.print_shape:\n",
    "            print('In', x.shape)\n",
    "        x = self.conv(x)\n",
    "        if self.print_shape:\n",
    "            print('After EncoderLayer', x.shape)\n",
    "        new_h = []\n",
    "        for i, res_bigru in enumerate(self.res_bigrus):\n",
    "            x, new_hi = res_bigru(x, h[i])\n",
    "            new_h.append(new_hi)\n",
    "        if self.print_shape:\n",
    "            print('After GRU', x.shape)\n",
    "        x = self.dconv(x.transpose(-1, -2))\n",
    "        if self.print_shape:\n",
    "            print('After DConv', x.shape)\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        x = x.transpose(-1,-2)\n",
    "            \n",
    "        if self.print_shape:\n",
    "            print('After SmoothConv', x.shape)\n",
    "        \n",
    "        return x, new_h  # probabilities + hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture\n",
    "arch = [\n",
    "    (2, 8, 2, 3),\n",
    "    (8, 32, 2, 3),\n",
    "    (32, 64, 2, 3)\n",
    "]\n",
    "\n",
    "in_channels = 2\n",
    "hidden_size = arch[-1][1]\n",
    "kernel_size = 20\n",
    "stride = arch[-1][0]\n",
    "dilation = 1\n",
    "n_layers = 3\n",
    "dconv_padding = 3\n",
    "len_mult = 2**len(arch)\n",
    "\n",
    "# Create the GRUNET model\n",
    "net = GRUNET(\n",
    "    arch=arch,\n",
    "    in_channels=in_channels,  # You didn't specify out_channels, so I assume it's 2.\n",
    "    kernel_size=kernel_size,\n",
    "    stride=stride,\n",
    "    dconv_padding=dconv_padding,\n",
    "    hidden_size=hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    bidir=True,\n",
    "    print_shape=True\n",
    ")\n",
    "\n",
    "# Generate random input data\n",
    "X = torch.randn(512, 2).float()\n",
    "\n",
    "# Forward pass through the network\n",
    "Z, h = net(X)\n",
    "\n",
    "# Check if input and output shapes match\n",
    "assert X.shape == Z.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the series_ids that are not in the drop_series list\n",
    "useable_series_ids = [s for s in series_ids if s not in drop_series]\n",
    "\n",
    "# If in DEMO_MODE (a configuration flag), limit the number of series IDs to 75 for quicker runs or testing\n",
    "if CFG.DEMO_MODE:\n",
    "    useable_series_ids = useable_series_ids[:75]\n",
    "\n",
    "# Shuffle the usable series IDs for randomness, which can be important for training/validation splits\n",
    "np.random.shuffle(useable_series_ids)\n",
    "\n",
    "# Print or retrieve the length of the usable series IDs\n",
    "len(useable_series_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigma(epoch):\n",
    "    \"\"\"\n",
    "    Determines the sigma value based on the current epoch.\n",
    "\n",
    "    The function returns different sigma values depending on the given epoch.\n",
    "    It's designed to change the sigma value as the training progresses through epochs.\n",
    "\n",
    "    Parameters:\n",
    "        epoch (int): The current epoch in the training process.\n",
    "\n",
    "    Returns:\n",
    "        int: The sigma value corresponding to the given epoch.\n",
    "    \"\"\"\n",
    "    # For early epochs (less than 4), return a larger sigma value\n",
    "    if epoch < 4:\n",
    "        return 90\n",
    "    # For middle epochs (4 to 6), return a medium sigma value\n",
    "    elif epoch < 7:\n",
    "        return 60\n",
    "    # For later epochs (7 and above), return a smaller sigma value\n",
    "    return 36\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Configuration\n",
    "NUM_FOLDS = 5\n",
    "LEARNING_RATE = .05\n",
    "CLIP_VALUE = 5.0\n",
    "WEIGHT_DECAY = 0.0\n",
    "WARMUP_PROP = 0.1\n",
    "EPOCHS = 10\n",
    "MAX_CHUNK_SIZE = 24 * 60 * 12\n",
    "SIGMA = 0\n",
    "valid_ds = None\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Splitting the entire dataset into 5 folds for cross-validation.\n",
    "for fold, valid_series_ids in enumerate(np.array_split(useable_series_ids, 5)):\n",
    "\n",
    "    print(f'Fold {fold}')\n",
    "    \n",
    "    train_series_ids = [s for s in useable_series_ids if s not in valid_series_ids]\n",
    "    \n",
    "    net = GRUNET(arch=arch,in_channels=2, hidden_size=hidden_size, kernel_size=kernel_size, stride=stride, \n",
    "                 dconv_padding=dconv_padding, n_layers=n_layers, bidir=True, print_shape=False).to(CFG.DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "    train_size = len(train_series_ids)\n",
    "    steps = train_size * EPOCHS\n",
    "    warmup_steps = int(steps * WARMUP_PROP)\n",
    "    scheduler = CosineLRScheduler(optimizer,t_initial= steps,warmup_t=warmup_steps, warmup_lr_init=1e-5,lr_min=1e-6,)\n",
    "    m = nn.LogSoftmax(dim=0)\n",
    "\n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    learning_rate_history = []\n",
    "    mAP_history = []\n",
    "\n",
    "    # with torch.autograd.detect_anomaly(check_nan=True):\n",
    "    for epoch in range(EPOCHS):\n",
    "        SIGMA = get_sigma(epoch)\n",
    "        \n",
    "        np.random.shuffle(train_series_ids)\n",
    "        train_ds = SleepDatasetTrain(train_series_ids, events=train_events, len_mult=len_mult, continuous=continuous, sigma=SIGMA)\n",
    "        valid_ds = SleepDatasetTrain(valid_series_ids, events=train_events, len_mult=len_mult, sigma=SIGMA)\n",
    "        print(f'Epoch {epoch}, sigma = {SIGMA}')\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for i in tqdm(range(len(train_ds))):\n",
    "            X, Y = train_ds[i]\n",
    "            Y = Y.to(CFG.DEVICE, non_blocking=True)\n",
    "            if not np.isfinite(Y.sum().cpu()):\n",
    "                print(f'Nan Target {i}')\n",
    "\n",
    "            pred = torch.zeros(Y.shape).to(CFG.DEVICE, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step(i+train_size*epoch)\n",
    "            h = None\n",
    "\n",
    "            seq_len = X.shape[0]\n",
    "            for j in range(0, seq_len, MAX_CHUNK_SIZE):\n",
    "                X_chunk = X[j: j + MAX_CHUNK_SIZE].float().to(CFG.DEVICE, non_blocking=True)\n",
    "                y_pred, h = net(X_chunk, h)\n",
    "                h = [hi.detach() for hi in h]\n",
    "                pred[j: j+MAX_CHUNK_SIZE, :] = y_pred\n",
    "\n",
    "                del X_chunk, y_pred\n",
    "\n",
    "            if not np.isfinite(pred.sum().cpu().detach()):\n",
    "                print(f'Nan Pred before logsoftmax {i}')\n",
    "            pred = m(pred.float())\n",
    "            if not np.isfinite(pred.sum().cpu().detach()):\n",
    "                print(f'Nan Pred after logsoftmax {i}')\n",
    "            loss = loss_fct(pred.float(), Y.float())\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), max_norm=CLIP_VALUE)\n",
    "            optimizer.step()\n",
    "\n",
    "            del pred, loss, Y, X, h\n",
    "            clean_memory()\n",
    "\n",
    "        # Calculating average training loss for the epoch.\n",
    "        train_loss /= len(train_ds)\n",
    "        print(f'Epoch {epoch} train loss = {train_loss}')\n",
    "        train_loss_history.append(train_loss)\n",
    "        print(f'Learning Rate = {optimizer.param_groups[0][\"lr\"]}')\n",
    "        learning_rate_history.append(optimizer.param_groups[0][\"lr\"])\n",
    "        print('Evaluate Validation Loss and mAP')\n",
    "        net.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(len(valid_ds))):\n",
    "                X, Y = valid_ds[i]\n",
    "                Y = Y.to(CFG.DEVICE, non_blocking=True)\n",
    "                pred = torch.zeros(Y.shape).to(CFG.DEVICE, non_blocking=True)\n",
    "\n",
    "                h = None\n",
    "\n",
    "                seq_len = X.shape[0]\n",
    "                for j in range(0, seq_len, MAX_CHUNK_SIZE):\n",
    "                    X_chunk = X[j: j + MAX_CHUNK_SIZE].float().to(CFG.DEVICE, non_blocking=True)\n",
    "                    y_pred, h = net(X_chunk, h)\n",
    "                    h = [hi.detach() for hi in h]\n",
    "                    pred[j: j+MAX_CHUNK_SIZE, :] = y_pred\n",
    "\n",
    "                    del X_chunk, y_pred\n",
    "                pred = m(pred.float())\n",
    "                loss = loss_fct(pred.float(), Y.float())\n",
    "                val_loss += loss.item()\n",
    "                del pred, loss, Y, X, h\n",
    "                clean_memory()\n",
    "            val_loss /= len(valid_ds)\n",
    "\n",
    "            if epoch >= 1:\n",
    "                all_df = []\n",
    "                all_truth_df = []\n",
    "                for i in tqdm(range(len(valid_ds))):\n",
    "                    series_id = valid_ds.series_ids[i]\n",
    "                    # print(series_id)\n",
    "                    data = valid_ds.load_data(series_id)\n",
    "                    res_df, act_df = compare_predictions(valid_ds, i, net)\n",
    "                    res_df['step'] = data['step']\n",
    "                    onset_pred = get_predictions(res_df, target='onset_val', SIGMA=SIGMA)\n",
    "                    wakeup_pred = get_predictions(res_df, target='wakeup_val', SIGMA=SIGMA)\n",
    "                    pred_df = pd.DataFrame(wakeup_pred + onset_pred, columns=['step', 'event', 'score'])\n",
    "                    pred_df['series_id'] = series_id\n",
    "                    pred_df['row_id'] = pred_df.index\n",
    "                    pred_df = pred_df.sort_values(by='step').drop_duplicates(subset='step').reset_index(drop=True)\n",
    "\n",
    "                    all_df.append(pred_df)\n",
    "                    all_truth_df.append(train_events[(train_events.series_id == series_id) & (train_events.step <= data.step.max()) & (train_events.step >= data.step.min())])\n",
    "\n",
    "                pred_df = pd.concat(all_df).reset_index(drop=True)\n",
    "                pred_df['row_id'] = pred_df.index\n",
    "                pred_df = pred_df[['row_id', 'series_id', 'step', 'event', 'score']]\n",
    "                pred_df = pred_df.sort_values(by=['series_id', 'step'])\n",
    "                pred_df.event = pred_df.event.map(lambda x: x.replace('_val', ''))\n",
    "                #pred_df = renormalize(pred_df)\n",
    "                truth_df = pd.concat(all_truth_df).reset_index(drop=True)\n",
    "                if len(pred_df) > 0:\n",
    "                    map_val = mapmetric.event_detection_ap(solution=truth_df, submission=pred_df[['series_id', 'step', 'event', 'score']], tolerances=tolerances)\n",
    "                else:\n",
    "                    print(f'Empty pred dataframe')\n",
    "                    map_val = 0\n",
    "\n",
    "                tmp = [x for x in mAP_history if not np.isnan(x)]\n",
    "                if len(tmp) > 0 and map_val > np.max(tmp):\n",
    "                    torch.save(net.state_dict(), f'model_best_mAP{fold}.pth')\n",
    "            else:\n",
    "                map_val = np.nan\n",
    "\n",
    "        print(f'Epoch {epoch} validation loss = {val_loss}, mAP = {map_val}')\n",
    "        valid_loss_history.append(val_loss)\n",
    "        mAP_history.append(map_val)\n",
    "        \n",
    "        \n",
    "    torch.save(net.state_dict(), f'model_resid_bigru_fold{fold}.pth')\n",
    "    iplot({'data': [Scatter(y=train_loss_history, name='train'), Scatter(y=valid_loss_history, name='valid')], 'layout': Layout(title=f'KLDivLoss {fold}')})\n",
    "    iplot({'data': [Scatter(y=learning_rate_history, name='lr')], 'layout': Layout(title=f'Learning Rate {fold}')})\n",
    "    iplot({'data': [Scatter(y=mAP_history, name='mAP')], 'layout': Layout(title=f'Event mAP {fold}')})\n",
    "    print('Break after 1 to save GPU!')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "series_id = valid_ds.series_ids[i]\n",
    "print(series_id)\n",
    "data = valid_ds.load_data(series_id)\n",
    "res_df, act_df = compare_predictions(valid_ds, i, net)\n",
    "res_df['step'] = data['step']\n",
    "act_df['step'] = data['step']\n",
    "\n",
    "iplot({'data': [Scatter(x=res_df['step'], y=res_df['wakeup_val'], name='pred'), Scatter(x=act_df['step'], y=act_df['wakeup_val'], name='act')], 'layout': Layout(title='wakeup')})\n",
    "iplot({'data': [Scatter(x=res_df['step'], y=res_df['onset_val'], name='pred'), Scatter(x=act_df['step'], y=act_df['onset_val'], name='act')], 'layout': Layout(title='onset')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleep_states",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
